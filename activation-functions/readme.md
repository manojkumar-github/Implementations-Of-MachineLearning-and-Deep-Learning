### Activation Functions
   
   Tensorflow:
   
 - Smooth Non-Linearities - sigmoid, tanh, elu, selu, softplus, and softsign
 - continuous but not everywhere differentiable functions (relu, relu6, crelu and relu_x)
 - random regularization (dropout)
 - bias_add
 
Keras:
 - softmax, elu, selu, softplus, softsign,relu,tanh, sigmoid, hard_sigmoid, linear
 - Advanced Activation Functions: LeakyReLU,PReLU,ELU, ThresholdedReLU, Softmax
